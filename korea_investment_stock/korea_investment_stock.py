'''
í•œêµ­íˆ¬ìì¦ê¶Œ python wrapper
'''
import datetime
import json
import os
import pickle
import random
import time
import zipfile
import logging
import re
from pathlib import Path
from typing import Literal, Optional, List
from zoneinfo import ZoneInfo  # Requires Python 3.9+
from datetime import datetime, timedelta

import pandas as pd
import requests
from typing import Dict, Any

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

EXCHANGE_CODE = {
    "í™ì½©": "HKS",
    "ë‰´ìš•": "NYS",
    "ë‚˜ìŠ¤ë‹¥": "NAS",
    "ì•„ë©•ìŠ¤": "AMS",
    "ë„ì¿„": "TSE",
    "ìƒí•´": "SHS",
    "ì‹¬ì²œ": "SZS",
    "ìƒí•´ì§€ìˆ˜": "SHI",
    "ì‹¬ì²œì§€ìˆ˜": "SZI",
    "í˜¸ì¹˜ë¯¼": "HSX",
    "í•˜ë…¸ì´": "HNX"
}

# í•´ì™¸ì£¼ì‹ ì£¼ë¬¸
# í•´ì™¸ì£¼ì‹ ì”ê³ 
EXCHANGE_CODE2 = {
    "ë¯¸êµ­ì „ì²´": "NASD",
    "ë‚˜ìŠ¤ë‹¥": "NAS",
    "ë‰´ìš•": "NYSE",
    "ì•„ë©•ìŠ¤": "AMEX",
    "í™ì½©": "SEHK",
    "ìƒí•´": "SHAA",
    "ì‹¬ì²œ": "SZAA",
    "ë„ì¿„": "TKSE",
    "í•˜ë…¸ì´": "HASE",
    "í˜¸ì¹˜ë¯¼": "VNSE"
}

EXCHANGE_CODE3 = {
    "ë‚˜ìŠ¤ë‹¥": "NASD",
    "ë‰´ìš•": "NYSE",
    "ì•„ë©•ìŠ¤": "AMEX",
    "í™ì½©": "SEHK",
    "ìƒí•´": "SHAA",
    "ì‹¬ì²œ": "SZAA",
    "ë„ì¿„": "TKSE",
    "í•˜ë…¸ì´": "HASE",
    "í˜¸ì¹˜ë¯¼": "VNSE"
}

EXCHANGE_CODE4 = {
    "ë‚˜ìŠ¤ë‹¥": "NAS",
    "ë‰´ìš•": "NYS",
    "ì•„ë©•ìŠ¤": "AMS",
    "í™ì½©": "HKS",
    "ìƒí•´": "SHS",
    "ì‹¬ì²œ": "SZS",
    "ë„ì¿„": "TSE",
    "í•˜ë…¸ì´": "HNX",
    "í˜¸ì¹˜ë¯¼": "HSX",
    "ìƒí•´ì§€ìˆ˜": "SHI",
    "ì‹¬ì²œì§€ìˆ˜": "SZI"
}

CURRENCY_CODE = {
    "ë‚˜ìŠ¤ë‹¥": "USD",
    "ë‰´ìš•": "USD",
    "ì•„ë©•ìŠ¤": "USD",
    "í™ì½©": "HKD",
    "ìƒí•´": "CNY",
    "ì‹¬ì²œ": "CNY",
    "ë„ì¿„": "JPY",
    "í•˜ë…¸ì´": "VND",
    "í˜¸ì¹˜ë¯¼": "VND"
}

MARKET_TYPE_MAP = {
    "KR": ["300"],  # "301", "302"
    "KRX": ["300"],  # "301", "302"
    "NASDAQ": ["512"],
    "NYSE": ["513"],
    "AMEX": ["529"],
    "US": ["512", "513", "529"],
    "TYO": ["515"],
    "JP": ["515"],
    "HKEX": ["501"],
    "HK": ["501", "543", "558"],
    "HNX": ["507"],
    "HSX": ["508"],
    "VN": ["507", "508"],
    "SSE": ["551"],
    "SZSE": ["552"],
    "CN": ["551", "552"]
}

MARKET_TYPE = Literal[
    "KRX",
    "NASDAQ",
    "NYSE",
    "AMEX",
    "TYO",
    "HKEX",
    "HNX",
    "HSX",
    "SSE",
    "SZSE",
]

EXCHANGE_TYPE = Literal[
    "NAS",
    "NYS",
    "AMS"
]

MARKET_CODE_MAP: dict[str, MARKET_TYPE] = {
    "300": "KRX",
    "301": "KRX",
    "302": "KRX",
    "512": "NASDAQ",
    "513": "NYSE",
    "529": "AMEX",
    "515": "TYO",
    "501": "HKEX",
    "543": "HKEX",
    "558": "HKEX",
    "507": "HNX",
    "508": "HSX",
    "551": "SSE",
    "552": "SZSE",
}

EXCHANGE_CODE_MAP: dict[str, EXCHANGE_TYPE] = {
    "NASDAQ": "NAS",
    "NYSE": "NYS",
    "AMEX": "AMS"
}

API_RETURN_CODE = {
    "SUCCESS": "0",  # ì¡°íšŒë˜ì—ˆìŠµë‹ˆë‹¤
    "EXPIRED_TOKEN": "1",  # ê¸°ê°„ì´ ë§Œë£Œëœ token ì…ë‹ˆë‹¤
    "NO_DATA": "7",  # ì¡°íšŒí•  ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤
    "RATE_LIMIT_EXCEEDED": "EGW00201",  # Rate limit ì´ˆê³¼
}


# Note: retry_on_rate_limit decoratorëŠ” enhanced_retry_decorator ëª¨ë“ˆì—ì„œ importë¨


class KoreaInvestment:
    '''
    í•œêµ­íˆ¬ìì¦ê¶Œ REST API
    '''

    def __init__(self, api_key: str, api_secret: str, acc_no: str, mock: bool = False):
        """í•œêµ­íˆ¬ìì¦ê¶Œ API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”

        Args:
            api_key (str): ë°œê¸‰ë°›ì€ API key
            api_secret (str): ë°œê¸‰ë°›ì€ API secret
            acc_no (str): ê³„ì¢Œë²ˆí˜¸ ì²´ê³„ì˜ ì• 8ìë¦¬-ë’¤ 2ìë¦¬ (ì˜ˆ: "12345678-01")
            mock (bool): True (mock trading), False (real trading)
        """
        self.mock = mock
        self.set_base_url(mock)
        self.api_key = api_key
        self.api_secret = api_secret

        # account number
        self.acc_no = acc_no
        self.acc_no_prefix = acc_no.split('-')[0]
        self.acc_no_postfix = acc_no.split('-')[1]

        # access token
        self.token_file = Path("~/.cache/mojito2/token.dat").expanduser()
        self.access_token = None
        if self.check_access_token():
            self.load_access_token()
        else:
            self.issue_access_token()

    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ - ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.shutdown()
        return False  # ì˜ˆì™¸ë¥¼ ì „íŒŒ

    def __handle_rate_limit_error(self, retry_count: int):
        """Rate limit ì—ëŸ¬ ì²˜ë¦¬ (Exponential Backoff)
        
        DEPRECATED: Enhanced Backoff Strategyë¡œ ëŒ€ì²´ë¨
        ì´ ë©”ì„œë“œëŠ” í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ë˜ë©°, í–¥í›„ ì œê±°ë  ì˜ˆì •ì…ë‹ˆë‹¤.
        
        Args:
            retry_count: ì¬ì‹œë„ íšŸìˆ˜ (0ë¶€í„° ì‹œì‘)
        """
        # Exponential backoff: 1, 2, 4, 8, 16, 32ì´ˆ
        wait_time = min(2 ** retry_count, 32)
        
        # Jitter ì¶”ê°€ (0~10% ëœë¤ ì¶”ê°€ ëŒ€ê¸°)
        jitter = random.uniform(0, 0.1 * wait_time)
        total_wait = wait_time + jitter
        
        print(f"Rate limit ì´ˆê³¼. {total_wait:.2f}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„... (ì‹œë„ {retry_count + 1}/5)")
        time.sleep(total_wait)

    def shutdown(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - ThreadPoolExecutor ì¢…ë£Œ"""
        if hasattr(self, 'executor') and self.executor:
            print("ThreadPoolExecutor ì¢…ë£Œ ì¤‘...")
            self.executor.shutdown(wait=True)
            self.executor = None
            print("ThreadPoolExecutor ì¢…ë£Œ ì™„ë£Œ")
        
        # Rate limiter í†µê³„ ìµœì¢… ì¶œë ¥ ë° ì €ì¥
        if hasattr(self, 'rate_limiter'):
            if hasattr(self.rate_limiter, 'get_stats'):
                stats = self.rate_limiter.get_stats()
                if stats.get('total_calls', 0) > 0:
                    print(f"\nìµœì¢… Rate Limiter í†µê³„:")
                    print(f"- ì´ í˜¸ì¶œ ìˆ˜: {stats['total_calls']}")
                    print(f"- ì—ëŸ¬ ìˆ˜: {stats['error_count']}")
                    print(f"- ì—ëŸ¬ìœ¨: {stats['error_rate']:.1%}")
            
            # í†µê³„ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            if hasattr(self.rate_limiter, 'save_stats'):
                filepath = self.rate_limiter.save_stats(include_timestamp=True)
                if filepath:
                    print(f"- í†µê³„ ì €ì¥ë¨: {filepath}")
            
            # ìë™ ì €ì¥ ë¹„í™œì„±í™”
            if hasattr(self.rate_limiter, 'disable_auto_save'):
                self.rate_limiter.disable_auto_save()
        
        # Backoff ì „ëµ í†µê³„ ì¶œë ¥
        backoff_strategy = get_backoff_strategy()
        backoff_stats = backoff_strategy.get_stats()
        if backoff_stats['total_attempts'] > 0:
            print(f"\nìµœì¢… Backoff ì „ëµ í†µê³„:")
            print(f"- Circuit ìƒíƒœ: {backoff_stats['state']}")
            print(f"- ì´ ì‹œë„: {backoff_stats['total_attempts']}")
            print(f"- ì´ ì‹¤íŒ¨: {backoff_stats['total_failures']}")
            print(f"- ì„±ê³µë¥ : {backoff_stats['success_rate']:.1%}")
            print(f"- Circuit Open íšŸìˆ˜: {backoff_stats['circuit_opens']}")
            print(f"- í‰ê·  ë°±ì˜¤í”„ ì‹œê°„: {backoff_stats['avg_backoff_time']:.2f}ì´ˆ")
        
        # ìºì‹œ í†µê³„ ì¶œë ¥ (Phase 8.7)
        if self._cache_enabled and self._cache:
            cache_stats = self.get_cache_stats()
            if cache_stats['total_entries'] > 0 or cache_stats['hit_count'] > 0:
                print(f"\nìµœì¢… ìºì‹œ í†µê³„:")
                print(f"- í™œì„±í™” ì—¬ë¶€: {'ì˜ˆ' if cache_stats['enabled'] else 'ì•„ë‹ˆì˜¤'}")
                print(f"- ì´ í•­ëª© ìˆ˜: {cache_stats['total_entries']}")
                print(f"- ìºì‹œ ì ì¤‘: {cache_stats['hit_count']}")
                print(f"- ìºì‹œ ë¯¸ìŠ¤: {cache_stats['miss_count']}")
                print(f"- ì ì¤‘ë¥ : {cache_stats['hit_rate']:.1%}")
                print(f"- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {cache_stats['memory_usage']:.1f}MB")
                print(f"- ë§Œë£Œëœ í•­ëª©: {cache_stats['expired_count']}")
                print(f"- ì œê±°ëœ í•­ëª©: {cache_stats['eviction_count']}")
        
        # ì—ëŸ¬ ë³µêµ¬ ì‹œìŠ¤í…œ í†µê³„ ì¶œë ¥
        recovery_system = get_error_recovery_system()
        error_summary = recovery_system.get_error_summary(hours=24)
        if error_summary['total_errors'] > 0:
            print(f"\nìµœì¢… ì—ëŸ¬ ë³µêµ¬ í†µê³„ (ìµœê·¼ 24ì‹œê°„):")
            print(f"- ì´ ì—ëŸ¬ ìˆ˜: {error_summary['total_errors']}")
            print(f"- ì‹¬ê°ë„ë³„ ë¶„í¬: {error_summary['by_severity']}")
            print(f"- ë³µêµ¬ ì„±ê³µë¥ : {error_summary['recovery_rate']:.1%}")
            print(f"- ê°€ì¥ ë¹ˆë²ˆí•œ ì—ëŸ¬:")
            for error_info in error_summary['most_common'][:3]:
                print(f"  - {error_info['error']}: {error_info['count']}íšŒ")
        
        # ì—ëŸ¬ í†µê³„ íŒŒì¼ë¡œ ì €ì¥
        recovery_system.save_stats()
        
        # í†µí•© í†µê³„ ì €ì¥ (Phase 5.1)
        print("\ní†µí•© í†µê³„ ì €ì¥ ì¤‘...")
        stats_manager = get_stats_manager()
        
        # DynamicBatchControllerê°€ ìˆë‹¤ë©´ í¬í•¨
        batch_controller = None
        if hasattr(self, '_dynamic_batch_controller'):
            batch_controller = self._dynamic_batch_controller
        
        # ëª¨ë“  ëª¨ë“ˆì˜ í†µê³„ ìˆ˜ì§‘
        all_stats = stats_manager.collect_all_stats(
            rate_limiter=self.rate_limiter if hasattr(self, 'rate_limiter') else None,
            backoff_strategy=backoff_strategy,
            error_recovery=recovery_system,
            batch_controller=batch_controller,
            cache=self._cache if self._cache_enabled and self._cache else None
        )
        
        # JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
        json_path = stats_manager.save_stats(all_stats, format='json', include_timestamp=True)
        print(f"- í†µí•© í†µê³„ ì €ì¥ë¨ (JSON): {json_path}")
        
        # CSV í˜•ì‹ìœ¼ë¡œë„ ì €ì¥ (ìš”ì•½ ì •ë³´)
        csv_path = stats_manager.save_stats(all_stats, format='csv', include_timestamp=True)
        print(f"- í†µí•© í†µê³„ ì €ì¥ë¨ (CSV): {csv_path}")
        
        # ì••ì¶•ëœ JSON Lines í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ì¥ê¸° ë³´ê´€ìš©)
        jsonl_gz_path = stats_manager.save_stats(
            all_stats, 
            format='jsonl', 
            compress=True,
            filename='stats_history',
            include_timestamp=False
        )
        print(f"- í†µê³„ ì´ë ¥ ì¶”ê°€ë¨ (JSONL.GZ): {jsonl_gz_path}")
        
        # ì‹œìŠ¤í…œ ìƒíƒœ ìš”ì•½ ì¶œë ¥
        summary = all_stats.get('summary', {})
        print(f"\nì‹œìŠ¤í…œ ìµœì¢… ìƒíƒœ: {summary.get('system_health', 'UNKNOWN')}")
        print(f"- ì „ì²´ API í˜¸ì¶œ: {summary.get('total_api_calls', 0):,}")
        print(f"- ì „ì²´ ì—ëŸ¬: {summary.get('total_errors', 0):,}")
        print(f"- ì „ì²´ ì—ëŸ¬ìœ¨: {summary.get('overall_error_rate', 0):.2%}")
        
        # ìºì‹œ ì •ë¦¬ (Phase 8.7)
        if self._cache_enabled and self._cache:
            # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì •ì§€
            if hasattr(self._cache, 'stop_cleanup_thread'):
                self._cache.stop_cleanup_thread()
            logger.info("ìºì‹œ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì •ë¦¬ ì™„ë£Œ")

    def set_base_url(self, mock: bool = True):
        """í…ŒìŠ¤íŠ¸(ëª¨ì˜íˆ¬ì) ì„œë²„ ì‚¬ìš© ì„¤ì •
        Args:
            mock(bool, optional): True: í…ŒìŠ¤íŠ¸ì„œë²„, False: ì‹¤ì„œë²„ Defaults to True.
        """
        if mock:
            self.base_url = "https://openapivts.koreainvestment.com:29443"
        else:
            self.base_url = "https://openapi.koreainvestment.com:9443"

    @retry_on_rate_limit(max_retries=3)  # í† í° ë°œê¸‰ì€ 3íšŒë§Œ ì¬ì‹œë„
    def issue_access_token(self):
        """OAuthì¸ì¦/ì ‘ê·¼í† í°ë°œê¸‰
        """
        path = "oauth2/tokenP"
        url = f"{self.base_url}/{path}"
        headers = {"content-type": "application/json"}
        data = {
            "grant_type": "client_credentials",
            "appkey": self.api_key,
            "appsecret": self.api_secret
        }

        resp = requests.post(url, headers=headers, json=data)
        resp_data = resp.json()
        self.access_token = f'Bearer {resp_data["access_token"]}'

        # 'expires_in' has no reference time and causes trouble:
        # The server thinks I'm expired but my token.dat looks still valid!
        # Hence, we use 'access_token_token_expired' here.
        # This error is quite big. I've seen 4000 seconds.
        timezone = ZoneInfo('Asia/Seoul')
        dt = datetime.strptime(resp_data['access_token_token_expired'], '%Y-%m-%d %H:%M:%S').replace(
            tzinfo=timezone)
        resp_data['timestamp'] = int(dt.timestamp())
        resp_data['api_key'] = self.api_key
        resp_data['api_secret'] = self.api_secret

        # dump access token
        self.token_file.parent.mkdir(parents=True, exist_ok=True)
        with self.token_file.open("wb") as f:
            pickle.dump(resp_data, f)

    def check_access_token(self) -> bool:
        """check access token

        Returns:
            Bool: True: token is valid, False: token is not valid
        """

        if not self.token_file.exists():
            return False

        with self.token_file.open("rb") as f:
            data = pickle.load(f)

        expire_epoch = data['timestamp']
        now_epoch = int(datetime.now().timestamp())
        status = False

        if (data['api_key'] != self.api_key) or (data['api_secret'] != self.api_secret):
            return False

        good_until = data['timestamp']
        ts_now = int(datetime.now().timestamp())
        return ts_now < good_until

    def load_access_token(self):
        """load access token
        """
        with self.token_file.open("rb") as f:
            data = pickle.load(f)
        self.access_token = f'Bearer {data["access_token"]}'

    def issue_hashkey(self, data: dict):
        """í•´ì‰¬í‚¤ ë°œê¸‰
        Args:
            data (dict): POST ìš”ì²­ ë°ì´í„°
        Returns:
            _type_: _description_
        """
        path = "uapi/hashkey"
        url = f"{self.base_url}/{path}"
        headers = {
            "content-type": "application/json",
            "appKey": self.api_key,
            "appSecret": self.api_secret,
            "User-Agent": "Mozilla/5.0"
        }
        resp = requests.post(url, headers=headers, data=json.dumps(data))
        haskkey = resp.json()["HASH"]
        return haskkey

    def __fetch_price(self, symbol: str, market: str = "KR") -> dict:
        """êµ­ë‚´ì£¼ì‹ì‹œì„¸/ì£¼ì‹í˜„ì¬ê°€ ì‹œì„¸
           í•´ì™¸ì£¼ì‹í˜„ì¬ê°€/í•´ì™¸ì£¼ì‹ í˜„ì¬ì²´ê²°ê°€

        Args:
            symbol (str): ì¢…ëª©ì½”ë“œ

        Returns:
            dict: _description_
        """

        if market == "KR" or market == "KRX":
            stock_info = self.__fetch_stock_info(symbol, market)
            symbol_type = self.__get_symbol_type(stock_info)
            if symbol_type == "ETF":
                resp_json = self.__fetch_etf_domestic_price("J", symbol)
            else:
                resp_json = self.__fetch_domestic_price("J", symbol)
        elif market == "US":
            # ê¸°ì¡´: resp_json = self.fetch_oversea_price(symbol)  # ë©”ì„œë“œ ì—†ìŒ
            # ê°œì„ : ì´ë¯¸ êµ¬í˜„ëœ __fetch_price_detail_oversea() í™œìš©
            resp_json = self.__fetch_price_detail_oversea(symbol, market)
            # ì°¸ê³ : ì´ APIëŠ” í˜„ì¬ê°€ ì™¸ì—ë„ PER, PBR, EPS, BPS ë“± ì¶”ê°€ ì •ë³´ ì œê³µ
        else:
            raise ValueError("Unsupported market type")

        return resp_json

    def __get_symbol_type(self, symbol_info):
        symbol_type = symbol_info['output']['prdt_clsf_name']
        if symbol_type == 'ì£¼ê¶Œ' or symbol_type == 'ìƒì¥REITS' or symbol_type == 'ì‚¬íšŒê°„ì ‘ìë³¸íˆ¬ìœµìíšŒì‚¬':
            return 'Stock'
        elif symbol_type == 'ETF':
            return 'ETF'

        return "Unknown"

    @cacheable(
        ttl=300,  # 5ë¶„
        key_generator=lambda self, market_code, symbol: f"fetch_etf_domestic_price:{market_code}:{symbol}"
    )
    @retry_on_rate_limit()
    def __fetch_etf_domestic_price(self, market_code: str, symbol: str) -> dict:
        """ì£¼ì‹í˜„ì¬ê°€ì‹œì„¸ (ë‚´ë¶€ ë©”ì„œë“œ)
        
        Note: ì´ ë©”ì„œë“œëŠ” ë‚´ë¶€ ì‚¬ìš©ì„ ìœ„í•œ private ë©”ì„œë“œì…ë‹ˆë‹¤. 
        ì‚¬ìš©ìëŠ” fetch_price_list() í†µí•© ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
        
        Args:
            market_code (str): ì‹œì¥ ë¶„ë¥˜ì½”ë“œ
            symbol (str): ì¢…ëª©ì½”ë“œ
        Returns:
            dict: API ê°œë°œ ê°€ì´ë“œ ì°¸ì¡°
        """
        path = "uapi/domestic-stock/v1/quotations/inquire-price"
        url = f"{self.base_url}/{path}"
        headers = {
            "content-type": "application/json",
            "authorization": self.access_token,
            "appKey": self.api_key,
            "appSecret": self.api_secret,
            "tr_id": "FHPST02400000"
        }
        params = {
            "fid_cond_mrkt_div_code": market_code,
            "fid_input_iscd": symbol
        }
        resp = requests.get(url, headers=headers, params=params)
        return resp.json()

    @cacheable(
        ttl=300,  # 5ë¶„
        key_generator=lambda self, market_code, symbol: f"fetch_domestic_price:{market_code}:{symbol}"
    )
    @retry_on_rate_limit()
    def __fetch_domestic_price(self, market_code: str, symbol: str) -> dict:
        """ì£¼ì‹í˜„ì¬ê°€ì‹œì„¸ (ë‚´ë¶€ ë©”ì„œë“œ)
        
        Note: ì´ ë©”ì„œë“œëŠ” ë‚´ë¶€ ì‚¬ìš©ì„ ìœ„í•œ private ë©”ì„œë“œì…ë‹ˆë‹¤. 
        ì‚¬ìš©ìëŠ” fetch_price_list() í†µí•© ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
        
        Args:
            market_code (str): ì‹œì¥ ë¶„ë¥˜ì½”ë“œ
            symbol (str): ì¢…ëª©ì½”ë“œ
        Returns:
            dict: API ê°œë°œ ê°€ì´ë“œ ì°¸ì¡°
        """
        path = "uapi/domestic-stock/v1/quotations/inquire-price"
        url = f"{self.base_url}/{path}"
        headers = {
            "content-type": "application/json",
            "authorization": self.access_token,
            "appKey": self.api_key,
            "appSecret": self.api_secret,
            "tr_id": "FHKST01010100"
        }
        params = {
            "fid_cond_mrkt_div_code": market_code,
            "fid_input_iscd": symbol
        }
        resp = requests.get(url, headers=headers, params=params)
        return resp.json()

    @cacheable(
        ttl=259200,  # 3ì¼
        key_generator=lambda self: "fetch_kospi_symbols"
    )
    def fetch_kospi_symbols(self):
        """ì½”ìŠ¤í”¼ ì¢…ëª© ì½”ë“œ

        ì‹¤ì œ í•„ìš”í•œ ì¢…ëª©: ST, RT, EF, IF

        ST	ì£¼ê¶Œ
        MF	ì¦ê¶Œíˆ¬ìíšŒì‚¬
        RT	ë¶€ë™ì‚°íˆ¬ìíšŒì‚¬
        SC	ì„ ë°•íˆ¬ìíšŒì‚¬
        IF	ì‚¬íšŒê°„ì ‘ìë³¸íˆ¬ìœµìíšŒì‚¬
        DR	ì£¼ì‹ì˜ˆíƒì¦ì„œ
        EW	ELW
        EF	ETF
        SW	ì‹ ì£¼ì¸ìˆ˜ê¶Œì¦ê¶Œ
        SR	ì‹ ì£¼ì¸ìˆ˜ê¶Œì¦ì„œ
        BC	ìˆ˜ìµì¦ê¶Œ
        FE	í•´ì™¸ETF
        FS	ì™¸êµ­ì£¼ê¶Œ


        Returns:
            DataFrame:
        """
        base_dir = os.getcwd()
        file_name = "kospi_code.mst.zip"
        url = "https://new.real.download.dws.co.kr/common/master/" + file_name
        self.download_master_file(base_dir, file_name, url)
        df = self.parse_kospi_master(base_dir)
        return df

    @cacheable(
        ttl=259200,  # 3ì¼
        key_generator=lambda self: "fetch_kosdaq_symbols"
    )
    def fetch_kosdaq_symbols(self):
        """ì½”ìŠ¤ë‹¥ ì¢…ëª© ì½”ë“œ

        Returns:
            DataFrame:
        """
        base_dir = os.getcwd()
        file_name = "kosdaq_code.mst.zip"
        url = "https://new.real.download.dws.co.kr/common/master/" + file_name
        self.download_master_file(base_dir, file_name, url)
        df = self.parse_kosdaq_master(base_dir)
        return df

    def fetch_symbols(self):
        """fetch symbols from the exchange

        Returns:
            pd.DataFrame: pandas dataframe
        """
        if self.exchange == "ì„œìš¸":  # todo: exchangeëŠ” ì œê±° ì˜ˆì •
            df = self.fetch_kospi_symbols()
            kospi_df = df[['ë‹¨ì¶•ì½”ë“œ', 'í•œê¸€ëª…', 'ê·¸ë£¹ì½”ë“œ']].copy()
            kospi_df['ì‹œì¥'] = 'ì½”ìŠ¤í”¼'

            df = self.fetch_kosdaq_symbols()
            kosdaq_df = df[['ë‹¨ì¶•ì½”ë“œ', 'í•œê¸€ëª…', 'ê·¸ë£¹ì½”ë“œ']].copy()
            kosdaq_df['ì‹œì¥'] = 'ì½”ìŠ¤ë‹¥'

            df = pd.concat([kospi_df, kosdaq_df], axis=0)

        return df

    def download_master_file(self, base_dir: str, file_name: str, url: str):
        """download master file

        Args:
            base_dir (str): download directory
            file_name (str: filename
            url (str): url
        """
        os.chdir(base_dir)

        # delete legacy master file
        if os.path.exists(file_name):
            os.remove(file_name)

        # download master file
        resp = requests.get(url)
        with open(file_name, "wb") as f:
            f.write(resp.content)

        # unzip
        kospi_zip = zipfile.ZipFile(file_name)
        kospi_zip.extractall()
        kospi_zip.close()

    def parse_kospi_master(self, base_dir: str):
        """parse kospi master file

        Args:
            base_dir (str): directory where kospi code exists

        Returns:
            _type_: _description_
        """
        file_name = base_dir + "/kospi_code.mst"
        tmp_fil1 = base_dir + "/kospi_code_part1.tmp"
        tmp_fil2 = base_dir + "/kospi_code_part2.tmp"

        wf1 = open(tmp_fil1, mode="w", encoding="cp949")
        wf2 = open(tmp_fil2, mode="w")

        with open(file_name, mode="r", encoding="cp949") as f:
            for row in f:
                rf1 = row[0:len(row) - 228]
                rf1_1 = rf1[0:9].rstrip()
                rf1_2 = rf1[9:21].rstrip()
                rf1_3 = rf1[21:].strip()
                wf1.write(rf1_1 + ',' + rf1_2 + ',' + rf1_3 + '\n')
                rf2 = row[-228:]
                wf2.write(rf2)

        wf1.close()
        wf2.close()

        part1_columns = ['ë‹¨ì¶•ì½”ë“œ', 'í‘œì¤€ì½”ë“œ', 'í•œê¸€ëª…']
        df1 = pd.read_csv(tmp_fil1, header=None, encoding='cp949', names=part1_columns)

        field_specs = [
            2, 1, 4, 4, 4,
            1, 1, 1, 1, 1,
            1, 1, 1, 1, 1,
            1, 1, 1, 1, 1,
            1, 1, 1, 1, 1,
            1, 1, 1, 1, 1,
            1, 9, 5, 5, 1,
            1, 1, 2, 1, 1,
            1, 2, 2, 2, 3,
            1, 3, 12, 12, 8,
            15, 21, 2, 7, 1,
            1, 1, 1, 1, 9,
            9, 9, 5, 9, 8,
            9, 3, 1, 1, 1
        ]

        part2_columns = [
            'ê·¸ë£¹ì½”ë“œ', 'ì‹œê°€ì´ì•¡ê·œëª¨', 'ì§€ìˆ˜ì—…ì¢…ëŒ€ë¶„ë¥˜', 'ì§€ìˆ˜ì—…ì¢…ì¤‘ë¶„ë¥˜', 'ì§€ìˆ˜ì—…ì¢…ì†Œë¶„ë¥˜',
            'ì œì¡°ì—…', 'ì €ìœ ë™ì„±', 'ì§€ë°°êµ¬ì¡°ì§€ìˆ˜ì¢…ëª©', 'KOSPI200ì„¹í„°ì—…ì¢…', 'KOSPI100',
            'KOSPI50', 'KRX', 'ETP', 'ELWë°œí–‰', 'KRX100',
            'KRXìë™ì°¨', 'KRXë°˜ë„ì²´', 'KRXë°”ì´ì˜¤', 'KRXì€í–‰', 'SPAC',
            'KRXì—ë„ˆì§€í™”í•™', 'KRXì² ê°•', 'ë‹¨ê¸°ê³¼ì—´', 'KRXë¯¸ë””ì–´í†µì‹ ', 'KRXê±´ì„¤',
            'Non1', 'KRXì¦ê¶Œ', 'KRXì„ ë°•', 'KRXì„¹í„°_ë³´í—˜', 'KRXì„¹í„°_ìš´ì†¡',
            'SRI', 'ê¸°ì¤€ê°€', 'ë§¤ë§¤ìˆ˜ëŸ‰ë‹¨ìœ„', 'ì‹œê°„ì™¸ìˆ˜ëŸ‰ë‹¨ìœ„', 'ê±°ë˜ì •ì§€',
            'ì •ë¦¬ë§¤ë§¤', 'ê´€ë¦¬ì¢…ëª©', 'ì‹œì¥ê²½ê³ ', 'ê²½ê³ ì˜ˆê³ ', 'ë¶ˆì„±ì‹¤ê³µì‹œ',
            'ìš°íšŒìƒì¥', 'ë½êµ¬ë¶„', 'ì•¡ë©´ë³€ê²½', 'ì¦ìêµ¬ë¶„', 'ì¦ê±°ê¸ˆë¹„ìœ¨',
            'ì‹ ìš©ê°€ëŠ¥', 'ì‹ ìš©ê¸°ê°„', 'ì „ì¼ê±°ë˜ëŸ‰', 'ì•¡ë©´ê°€', 'ìƒì¥ì¼ì',
            'ìƒì¥ì£¼ìˆ˜', 'ìë³¸ê¸ˆ', 'ê²°ì‚°ì›”', 'ê³µëª¨ê°€', 'ìš°ì„ ì£¼',
            'ê³µë§¤ë„ê³¼ì—´', 'ì´ìƒê¸‰ë“±', 'KRX300', 'KOSPI', 'ë§¤ì¶œì•¡',
            'ì˜ì—…ì´ìµ', 'ê²½ìƒì´ìµ', 'ë‹¹ê¸°ìˆœì´ìµ', 'ROE', 'ê¸°ì¤€ë…„ì›”',
            'ì‹œê°€ì´ì•¡', 'ê·¸ë£¹ì‚¬ì½”ë“œ', 'íšŒì‚¬ì‹ ìš©í•œë„ì´ˆê³¼', 'ë‹´ë³´ëŒ€ì¶œê°€ëŠ¥', 'ëŒ€ì£¼ê°€ëŠ¥'
        ]

        df2 = pd.read_fwf(tmp_fil2, widths=field_specs, names=part2_columns)
        df = pd.merge(df1, df2, how='outer', left_index=True, right_index=True)

        # clean temporary file and dataframe
        del (df1)
        del (df2)
        os.remove(tmp_fil1)
        os.remove(tmp_fil2)
        return df

    def parse_kosdaq_master(self, base_dir: str):
        """parse kosdaq master file

        Args:
            base_dir (str): directory where kosdaq code exists

        Returns:
            _type_: _description_
        """
        file_name = base_dir + "/kosdaq_code.mst"
        tmp_fil1 = base_dir + "/kosdaq_code_part1.tmp"
        tmp_fil2 = base_dir + "/kosdaq_code_part2.tmp"

        wf1 = open(tmp_fil1, mode="w", encoding="cp949")
        wf2 = open(tmp_fil2, mode="w")
        with open(file_name, mode="r", encoding="cp949") as f:
            for row in f:
                rf1 = row[0:len(row) - 222]
                rf1_1 = rf1[0:9].rstrip()
                rf1_2 = rf1[9:21].rstrip()
                rf1_3 = rf1[21:].strip()
                wf1.write(rf1_1 + ',' + rf1_2 + ',' + rf1_3 + '\n')

                rf2 = row[-222:]
                wf2.write(rf2)

        wf1.close()
        wf2.close()

        part1_columns = ['ë‹¨ì¶•ì½”ë“œ', 'í‘œì¤€ì½”ë“œ', 'í•œê¸€ëª…']
        df1 = pd.read_csv(tmp_fil1, header=None, encoding="cp949", names=part1_columns)

        field_specs = [
            2, 1, 4, 4, 4,  # line 20
            1, 1, 1, 1, 1,  # line 27
            1, 1, 1, 1, 1,  # line 32
            1, 1, 1, 1, 1,  # line 38
            1, 1, 1, 1, 1,  # line 43
            1, 9, 5, 5, 1,  # line 48
            1, 1, 2, 1, 1,  # line 54
            1, 2, 2, 2, 3,  # line 64
            1, 3, 12, 12, 8,  # line 69
            15, 21, 2, 7, 1,  # line 75
            1, 1, 1, 9, 9,  # line 80
            9, 5, 9, 8, 9,  # line 85
            3, 1, 1, 1
        ]

        part2_columns = [
            'ê·¸ë£¹ì½”ë“œ', 'ì‹œê°€ì´ì•¡ê·œëª¨', 'ì§€ìˆ˜ì—…ì¢…ëŒ€ë¶„ë¥˜', 'ì§€ìˆ˜ì—…ì¢…ì¤‘ë¶„ë¥˜', 'ì§€ìˆ˜ì—…ì¢…ì†Œë¶„ë¥˜',  # line 20
            'ë²¤ì²˜ê¸°ì—…', 'ì €ìœ ë™ì„±', 'KRX', 'ETP', 'KRX100',  # line 27
            'KRXìë™ì°¨', 'KRXë°˜ë„ì²´', 'KRXë°”ì´ì˜¤', 'KRXì€í–‰', 'SPAC',  # line 32
            'KRXì—ë„ˆì§€í™”í•™', 'KRXì² ê°•', 'ë‹¨ê¸°ê³¼ì—´', 'KRXë¯¸ë””ì–´í†µì‹ ', 'KRXê±´ì„¤',  # line 38
            'íˆ¬ìì£¼ì˜', 'KRXì¦ê¶Œ', 'KRXì„ ë°•', 'KRXì„¹í„°_ë³´í—˜', 'KRXì„¹í„°_ìš´ì†¡',  # line 43
            'KOSDAQ150', 'ê¸°ì¤€ê°€', 'ë§¤ë§¤ìˆ˜ëŸ‰ë‹¨ìœ„', 'ì‹œê°„ì™¸ìˆ˜ëŸ‰ë‹¨ìœ„', 'ê±°ë˜ì •ì§€',  # line 48
            'ì •ë¦¬ë§¤ë§¤', 'ê´€ë¦¬ì¢…ëª©', 'ì‹œì¥ê²½ê³ ', 'ê²½ê³ ì˜ˆê³ ', 'ë¶ˆì„±ì‹¤ê³µì‹œ',  # line 54
            'ìš°íšŒìƒì¥', 'ë½êµ¬ë¶„', 'ì•¡ë©´ë³€ê²½', 'ì¦ìêµ¬ë¶„', 'ì¦ê±°ê¸ˆë¹„ìœ¨',  # line 64
            'ì‹ ìš©ê°€ëŠ¥', 'ì‹ ìš©ê¸°ê°„', 'ì „ì¼ê±°ë˜ëŸ‰', 'ì•¡ë©´ê°€', 'ìƒì¥ì¼ì',  # line 69
            'ìƒì¥ì£¼ìˆ˜', 'ìë³¸ê¸ˆ', 'ê²°ì‚°ì›”', 'ê³µëª¨ê°€', 'ìš°ì„ ì£¼',  # line 75
            'ê³µë§¤ë„ê³¼ì—´', 'ì´ìƒê¸‰ë“±', 'KRX300', 'ë§¤ì¶œì•¡', 'ì˜ì—…ì´ìµ',  # line 80
            'ê²½ìƒì´ìµ', 'ë‹¹ê¸°ìˆœì´ìµ', 'ROE', 'ê¸°ì¤€ë…„ì›”', 'ì‹œê°€ì´ì•¡',  # line 85
            'ê·¸ë£¹ì‚¬ì½”ë“œ', 'íšŒì‚¬ì‹ ìš©í•œë„ì´ˆê³¼', 'ë‹´ë³´ëŒ€ì¶œê°€ëŠ¥', 'ëŒ€ì£¼ê°€ëŠ¥'
        ]

        df2 = pd.read_fwf(tmp_fil2, widths=field_specs, names=part2_columns)
        df = pd.merge(df1, df2, how='outer', left_index=True, right_index=True)

        # clean temporary file and dataframe
        del (df1)
        del (df2)
        os.remove(tmp_fil1)
        os.remove(tmp_fil2)
        return df

    @cacheable(
        ttl=300,  # 5ë¶„ 
        key_generator=lambda self, symbol, market: f"fetch_price_detail_oversea:{market}:{symbol}"
    )
    @retry_on_rate_limit()
    def __fetch_price_detail_oversea(self, symbol: str, market: str = "KR"):
        """í•´ì™¸ì£¼ì‹ í˜„ì¬ê°€ìƒì„¸

        Args:
            symbol (str): symbol
        """
        self.rate_limiter.acquire()

        path = "/uapi/overseas-price/v1/quotations/price-detail"
        url = f"{self.base_url}/{path}"

        headers = {
            "content-type": "application/json",
            "authorization": self.access_token,
            "appKey": self.api_key,
            "appSecret": self.api_secret,
            "tr_id": "HHDFS76200200"
        }

        if market == "KR" or market == "KRX":
            # API í˜¸ì¶œí•´ì„œ ì‹¤ì œë¡œ í™•ì¸ì€ ëª»í•´ë´„, overasea ì´ë¼ì„œ ì•ˆë  ê²ƒìœ¼ë¡œ íŒë‹¨í•´ì„œ ì¡°ê±´ë¬¸ ì¶”ê°€í•¨
            raise ValueError("Market cannot be either 'KR' or 'KRX'.")

        for market_code in MARKET_TYPE_MAP[market]:
            print("market_code", market_code)
            market_type = MARKET_CODE_MAP[market_code]
            exchange_code = EXCHANGE_CODE_MAP[market_type]
            params = {
                "AUTH": "",
                "EXCD": exchange_code,
                "SYMB": symbol
            }
            resp = requests.get(url, headers=headers, params=params)
            resp_json = resp.json()
            if resp_json['rt_cd'] != API_RETURN_CODE["SUCCESS"] or resp_json['output']['rsym'] == '':
                continue

            return resp_json
        
        # ëª¨ë“  ê±°ë˜ì†Œì—ì„œ ì‹¤íŒ¨í•œ ê²½ìš°
        raise ValueError(f"Unable to fetch price for symbol '{symbol}' in any {market} exchange")

    @cacheable(
        ttl=18000,  # 5ì‹œê°„
        key_generator=lambda self, symbol, market: f"fetch_stock_info:{market}:{symbol}"
    )
    @retry_on_rate_limit()
    def __fetch_stock_info(self, symbol: str, market: str = "KR"):
        self.rate_limiter.acquire()

        path = "uapi/domestic-stock/v1/quotations/search-info"
        url = f"{self.base_url}/{path}"
        headers = {
            "content-type": "application/json",
            "authorization": self.access_token,
            "appKey": self.api_key,
            "appSecret": self.api_secret,
            "tr_id": "CTPF1604R"
        }

        for market_code in MARKET_TYPE_MAP[market]:
            try:
                params = {
                    "PDNO": symbol,
                    "PRDT_TYPE_CD": market_code
                }
                resp = requests.get(url, headers=headers, params=params)
                resp_json = resp.json()

                if resp_json['rt_cd'] == API_RETURN_CODE['NO_DATA']:
                    continue
                return resp_json

            except Exception as e:
                print(e)
                if resp_json['rt_cd'] != API_RETURN_CODE['SUCCESS']:
                    continue
                raise e

    @cacheable(
        ttl=18000,  # 5ì‹œê°„
        key_generator=lambda self, symbol, market: f"fetch_search_stock_info:{market}:{symbol}"
    )
    @retry_on_rate_limit()
    def __fetch_search_stock_info(self, symbol: str, market: str = "KR"):
        """
        êµ­ë‚´ ì£¼ì‹ë§Œ ì œê³µí•˜ëŠ” APIì´ë‹¤
        """

        self.rate_limiter.acquire()

        path = "uapi/domestic-stock/v1/quotations/search-stock-info"
        url = f"{self.base_url}/{path}"
        headers = {
            "content-type": "application/json",
            "authorization": self.access_token,
            "appKey": self.api_key,
            "appSecret": self.api_secret,
            "tr_id": "CTPF1002R"
        }

        if market != "KR" and market != "KRX":
            raise ValueError("Market must be either 'KR' or 'KRX'.")

        for market_ in MARKET_TYPE_MAP[market]:
            try:
                params = {
                    "PDNO": symbol,
                    "PRDT_TYPE_CD": market_
                }
                resp = requests.get(url, headers=headers, params=params)
                resp_json = resp.json()

                if resp_json['rt_cd'] == API_RETURN_CODE['NO_DATA']:
                    continue
                return resp_json

            except Exception as e:
                print(e)
                if resp_json['rt_cd'] != API_RETURN_CODE['SUCCESS']:
                    continue
                raise e

    
    # Phase 8.6: ìºì‹œ ê´€ë¦¬ ë©”ì„œë“œ
    def clear_cache(self, pattern: Optional[str] = None):
        """ìºì‹œ ì‚­ì œ
        
        Args:
            pattern: ì‚­ì œí•  ìºì‹œ í‚¤ íŒ¨í„´ (Noneì´ë©´ ì „ì²´ ì‚­ì œ)
                    ì˜ˆ: "fetch_domestic_price:J:005930"
        """
        if not self._cache_enabled or not self._cache:
            return
        
        if pattern is None:
            # ì „ì²´ ìºì‹œ ì‚­ì œ
            self._cache.clear()
            logger.info("ì „ì²´ ìºì‹œ ì‚­ì œ ì™„ë£Œ")
        else:
            # íŒ¨í„´ì— ë§ëŠ” ìºì‹œ ì‚­ì œ
            deleted_count = self._cache.delete_pattern(pattern)
            logger.info(f"{pattern} íŒ¨í„´ì˜ ìºì‹œ {deleted_count}ê°œ ì‚­ì œ ì™„ë£Œ")
    
    def get_cache_stats(self) -> dict:
        """ìºì‹œ í†µê³„ ì¡°íšŒ
        
        Returns:
            dict: ìºì‹œ í†µê³„ ì •ë³´
        """
        if not self._cache_enabled or not self._cache:
            return {
                'enabled': False,
                'hit_rate': 0.0,
                'total_entries': 0,
                'memory_usage': 0,
                'expired_count': 0
            }
        
        stats = self._cache.get_stats()
        return {
            'enabled': True,
            'hit_rate': stats.get('hit_rate', 0.0),
            'total_entries': stats.get('size', 0),
            'memory_usage': stats.get('memory_usage_mb', 0),
            'expired_count': stats.get('expired_count', 0),
            'hit_count': stats.get('hit_count', 0),
            'miss_count': stats.get('miss_count', 0),
            'eviction_count': stats.get('eviction_count', 0)
        }
    
    def set_cache_enabled(self, enabled: bool):
        """ìºì‹œ ê¸°ëŠ¥ on/off
        
        Args:
            enabled: Trueë©´ ìºì‹œ í™œì„±í™”, Falseë©´ ë¹„í™œì„±í™”
        """
        self._cache_enabled = enabled
        logger.info(f"ìºì‹œ {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")
    
    def preload_cache(self, symbols: List[str], market: str = "KR"):
        """ìì£¼ ì‚¬ìš©í•˜ëŠ” ì¢…ëª© ë¯¸ë¦¬ ìºì‹±
        
        Args:
            symbols: ì¢…ëª© ì½”ë“œ ë¦¬ìŠ¤íŠ¸
            market: ì‹œì¥ ì½”ë“œ (ê¸°ë³¸ê°’: "KR")
        """
        if not self._cache_enabled or not self._cache:
            logger.warning("ìºì‹œê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ preloadë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        print(f"ğŸ”„ {len(symbols)}ê°œ ì¢…ëª© ìºì‹œ ì‚¬ì „ ë¡œë“œ ì‹œì‘...")
        
        # ì¢…ëª© ì •ë³´ ë¡œë“œ
        stock_info_list = [(symbol, market) for symbol in symbols]
        self.fetch_stock_info_list(stock_info_list)
        
        # í˜„ì¬ê°€ ì •ë³´ ë¡œë“œ
        price_list = [(symbol, market) for symbol in symbols]
        self.fetch_price_list(price_list)
        
        print(f"âœ… {len(symbols)}ê°œ ì¢…ëª© ìºì‹œ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ")
        
        # ìºì‹œ í†µê³„ ì¶œë ¥
        stats = self.get_cache_stats()
        print(f"ğŸ“Š ìºì‹œ ìƒíƒœ: {stats['total_entries']}ê°œ í•­ëª©, "
              f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {stats['memory_usage']:.1f}MB")
    
    # Visualization ë©”ì„œë“œë“¤
    def create_monitoring_dashboard(self, 
                                  stats_dir: str = "logs/integrated_stats",
                                  update_interval: int = 5000) -> Optional[Any]:
        """ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ìƒì„±
        
        Args:
            stats_dir: í†µê³„ íŒŒì¼ ë””ë ‰í† ë¦¬
            update_interval: ì—…ë°ì´íŠ¸ ê°„ê²© (ë°€ë¦¬ì´ˆ)
            
        Returns:
            ëŒ€ì‹œë³´ë“œ Figure ê°ì²´ ë˜ëŠ” None
        """
        if not self.dashboard_manager:
            logger.error("Visualization ëª¨ë“ˆì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            # ë°ì´í„° ë¡œë“œ
            self.visualizer.stats_dir = Path(stats_dir)
            self.visualizer.load_history_data()
            self.visualizer.load_latest_stats()
            
            # ëŒ€ì‹œë³´ë“œ ìƒì„±
            dashboard = self.dashboard_manager.create_realtime_dashboard(update_interval)
            
            if dashboard:
                logger.info("ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ìƒì„± ì™„ë£Œ")
            
            return dashboard
            
        except Exception as e:
            logger.error(f"ëŒ€ì‹œë³´ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def save_monitoring_dashboard(self, 
                                filename: str = "api_monitoring_dashboard.html") -> bool:
        """ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            filename: ì €ì¥í•  íŒŒì¼ëª…
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not self.dashboard_manager:
            logger.error("Visualization ëª¨ë“ˆì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            path = self.dashboard_manager.save_dashboard(filename)
            return bool(path)
        except Exception as e:
            logger.error(f"ëŒ€ì‹œë³´ë“œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def create_stats_report(self, save_as: str = "monitoring_report") -> Dict[str, str]:
        """í†µê³„ ë¦¬í¬íŠ¸ ìƒì„±
        
        Args:
            save_as: ì €ì¥í•  íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)
            
        Returns:
            ìƒì„±ëœ íŒŒì¼ ê²½ë¡œë“¤
        """
        if not self.dashboard_manager:
            logger.error("Visualization ëª¨ë“ˆì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}
        
        try:
            paths = self.dashboard_manager.create_report(save_as)
            logger.info(f"í†µê³„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {len(paths)}ê°œ íŒŒì¼")
            return paths
        except Exception as e:
            logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def get_system_health_chart(self) -> Optional[Any]:
        """ì‹œìŠ¤í…œ í—¬ìŠ¤ ì°¨íŠ¸ ìƒì„±
        
        Returns:
            í—¬ìŠ¤ ì¸ë””ì¼€ì´í„° Figure ë˜ëŠ” None
        """
        if not self.visualizer:
            logger.error("Visualization ëª¨ë“ˆì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            # ìµœì‹  í†µê³„ ë¡œë“œ
            if not self.visualizer.latest_stats:
                self.visualizer.load_latest_stats()
            
            # í—¬ìŠ¤ ì°¨íŠ¸ ìƒì„±
            chart = self.visualizer.create_system_health_indicator()
            return chart
        except Exception as e:
            logger.error(f"í—¬ìŠ¤ ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def get_api_usage_chart(self, hours: int = 24) -> Optional[Any]:
        """API ì‚¬ìš©ëŸ‰ ì°¨íŠ¸ ìƒì„±
        
        Args:
            hours: í‘œì‹œí•  ì‹œê°„ ë²”ìœ„
            
        Returns:
            API ì‚¬ìš©ëŸ‰ ì°¨íŠ¸ Figure ë˜ëŠ” None
        """
        if not self.visualizer:
            logger.error("Visualization ëª¨ë“ˆì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            # íˆìŠ¤í† ë¦¬ ë°ì´í„° ë¡œë“œ
            if not self.visualizer.history_data:
                self.visualizer.load_history_data()
            
            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = self.visualizer.prepare_dataframe()
            
            # ì‹œê°„ í•„í„°ë§
            if not df.empty and 'timestamp' in df.columns:
                from datetime import datetime, timedelta
                cutoff_time = datetime.now() - timedelta(hours=hours)
                df = df[df['timestamp'] >= cutoff_time]
            
            # API í˜¸ì¶œ ì°¨íŠ¸ ìƒì„±
            chart = self.visualizer.create_api_calls_chart(df)
            return chart
        except Exception as e:
            logger.error(f"API ì‚¬ìš©ëŸ‰ ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def show_monitoring_dashboard(self):
        """ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ í‘œì‹œ (ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°)"""
        if not self.dashboard_manager:
            logger.error("Visualization ëª¨ë“ˆì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # ëŒ€ì‹œë³´ë“œê°€ ì—†ìœ¼ë©´ ìƒì„±
            if not self.dashboard_manager.dashboard:
                self.create_monitoring_dashboard()
            
            # ëŒ€ì‹œë³´ë“œ í‘œì‹œ
            self.dashboard_manager.show_dashboard()
        except Exception as e:
            logger.error(f"ëŒ€ì‹œë³´ë“œ í‘œì‹œ ì‹¤íŒ¨: {e}")

    # IPO ê´€ë ¨ í—¬í¼ í•¨ìˆ˜ë“¤
    def _validate_date_format(self, date_str: str) -> bool:
        """ë‚ ì§œ í˜•ì‹ ê²€ì¦ (YYYYMMDD)"""
        if len(date_str) != 8:
            return False
        try:
            datetime.strptime(date_str, "%Y%m%d")
            return True
        except ValueError:
            return False

    def _validate_date_range(self, from_date: str, to_date: str) -> bool:
        """ë‚ ì§œ ë²”ìœ„ ìœ íš¨ì„± ê²€ì¦"""
        try:
            start = datetime.strptime(from_date, "%Y%m%d")
            end = datetime.strptime(to_date, "%Y%m%d")
            return start <= end
        except ValueError:
            return False

    @staticmethod
    def parse_ipo_date_range(date_range_str: str) -> tuple:
        """ì²­ì•½ê¸°ê°„ ë¬¸ìì—´ íŒŒì‹±
        
        Args:
            date_range_str: "2024.01.15~2024.01.16" í˜•ì‹ì˜ ë¬¸ìì—´
            
        Returns:
            tuple: (ì‹œì‘ì¼ datetime, ì¢…ë£Œì¼ datetime) ë˜ëŠ” (None, None)
        """
        if not date_range_str:
            return (None, None)
        
        # "2024.01.15~2024.01.16" í˜•ì‹ íŒŒì‹±
        pattern = r'(\d{4}\.\d{2}\.\d{2})~(\d{4}\.\d{2}\.\d{2})'
        match = re.match(pattern, date_range_str)
        
        if match:
            try:
                start_str = match.group(1).replace('.', '')
                end_str = match.group(2).replace('.', '')
                start_date = datetime.strptime(start_str, "%Y%m%d")
                end_date = datetime.strptime(end_str, "%Y%m%d")
                return (start_date, end_date)
            except ValueError:
                pass
        
        return (None, None)

    @staticmethod
    def format_ipo_date(date_str: str) -> str:
        """ë‚ ì§œ í˜•ì‹ ë³€í™˜ (YYYYMMDD -> YYYY-MM-DD)"""
        if len(date_str) == 8:
            return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        elif '.' in date_str:
            return date_str.replace('.', '-')
        return date_str

    @staticmethod
    def calculate_ipo_d_day(ipo_date_str: str) -> int:
        """ì²­ì•½ì¼ê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜ ê³„ì‚°"""
        if '~' in ipo_date_str:
            start_date, _ = KoreaInvestment.parse_ipo_date_range(ipo_date_str)
            if start_date:
                today = datetime.now()
                return (start_date - today).days
        return -999

    @staticmethod
    def get_ipo_status(subscr_dt: str) -> str:
        """ì²­ì•½ ìƒíƒœ íŒë‹¨
        
        Returns:
            str: "ì˜ˆì •", "ì§„í–‰ì¤‘", "ë§ˆê°", "ì•Œìˆ˜ì—†ìŒ"
        """
        start_date, end_date = KoreaInvestment.parse_ipo_date_range(subscr_dt)
        if not start_date or not end_date:
            return "ì•Œìˆ˜ì—†ìŒ"
        
        today = datetime.now()
        if today < start_date:
            return "ì˜ˆì •"
        elif start_date <= today <= end_date:
            return "ì§„í–‰ì¤‘"
        else:
            return "ë§ˆê°"

    @staticmethod
    def format_number(num_str: str) -> str:
        """ìˆ«ì ë¬¸ìì—´ì— ì²œë‹¨ìœ„ ì½¤ë§ˆ ì¶”ê°€"""
        try:
            return f"{int(num_str):,}"
        except (ValueError, TypeError):
            return num_str

    # IPO Schedule API
    @cacheable(
        ttl=3600,  # 1ì‹œê°„
        key_generator=lambda self, from_date=None, to_date=None, symbol="": f"fetch_ipo_schedule:{from_date or 'DEFAULT'}:{to_date or 'DEFAULT'}:{symbol or 'ALL'}"
    )
    @retry_on_rate_limit()
    def fetch_ipo_schedule(self, from_date: str = None, to_date: str = None, symbol: str = "") -> dict:
        """ê³µëª¨ì£¼ ì²­ì•½ ì¼ì • ì¡°íšŒ
        
        ì˜ˆíƒì›ì •ë³´(ê³µëª¨ì£¼ì²­ì•½ì¼ì •) APIë¥¼ í†µí•´ ê³µëª¨ì£¼ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
        í•œêµ­íˆ¬ì HTS(eFriend Plus) > [0667] ê³µëª¨ì£¼ì²­ì•½ í™”ë©´ê³¼ ë™ì¼í•œ ê¸°ëŠ¥ì…ë‹ˆë‹¤.
        
        Args:
            from_date: ì¡°íšŒ ì‹œì‘ì¼ (YYYYMMDD, ê¸°ë³¸ê°’: ì˜¤ëŠ˜)
            to_date: ì¡°íšŒ ì¢…ë£Œì¼ (YYYYMMDD, ê¸°ë³¸ê°’: 30ì¼ í›„)
            symbol: ì¢…ëª©ì½”ë“œ (ì„ íƒ, ê³µë°±ì‹œ ì „ì²´ ì¡°íšŒ)
            
        Returns:
            dict: ê³µëª¨ì£¼ ì²­ì•½ ì¼ì • ì •ë³´
                {
                    "rt_cd": "0",  # ì„±ê³µì—¬ë¶€
                    "msg_cd": "ì‘ë‹µì½”ë“œ",
                    "msg1": "ì‘ë‹µë©”ì‹œì§€",
                    "output1": [
                        {
                            "record_date": "ê¸°ì¤€ì¼",
                            "sht_cd": "ì¢…ëª©ì½”ë“œ",
                            "isin_name": "ì¢…ëª©ëª…",
                            "fix_subscr_pri": "ê³µëª¨ê°€",
                            "face_value": "ì•¡ë©´ê°€",
                            "subscr_dt": "ì²­ì•½ê¸°ê°„",  # "2024.01.15~2024.01.16"
                            "pay_dt": "ë‚©ì…ì¼",
                            "refund_dt": "í™˜ë¶ˆì¼",
                            "list_dt": "ìƒì¥/ë“±ë¡ì¼",
                            "lead_mgr": "ì£¼ê°„ì‚¬",
                            "pub_bf_cap": "ê³µëª¨ì „ìë³¸ê¸ˆ",
                            "pub_af_cap": "ê³µëª¨í›„ìë³¸ê¸ˆ",
                            "assign_stk_qty": "ë‹¹ì‚¬ë°°ì •ë¬¼ëŸ‰"
                        }
                    ]
                }
                
        Raises:
            ValueError: ëª¨ì˜íˆ¬ì ì‚¬ìš©ì‹œ ë˜ëŠ” ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜ì‹œ
            
        Note:
            - ëª¨ì˜íˆ¬ìëŠ” ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
            - ì˜ˆíƒì›ì—ì„œ ì œê³µí•œ ìë£Œì´ë¯€ë¡œ ì •ë³´ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
            - ì‹¤ì œ ì²­ì•½ì‹œì—ëŠ” ë°˜ë“œì‹œ ê³µì‹ ê³µëª¨ì£¼ ì²­ì•½ ê³µê³ ë¬¸ì„ í™•ì¸í•˜ì„¸ìš”.
            
        Examples:
            >>> # ì „ì²´ ê³µëª¨ì£¼ ì¡°íšŒ (ì˜¤ëŠ˜ë¶€í„° 30ì¼)
            >>> ipos = broker.fetch_ipo_schedule()
            
            >>> # íŠ¹ì • ê¸°ê°„ ì¡°íšŒ
            >>> ipos = broker.fetch_ipo_schedule(
            ...     from_date="20240101",
            ...     to_date="20240131"
            ... )
            
            >>> # íŠ¹ì • ì¢…ëª© ì¡°íšŒ
            >>> ipo = broker.fetch_ipo_schedule(symbol="123456")
        """
        # ëª¨ì˜íˆ¬ì ì²´í¬
        if self.mock:
            raise ValueError("ê³µëª¨ì£¼ì²­ì•½ì¼ì • ì¡°íšŒëŠ” ëª¨ì˜íˆ¬ìë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
        self.rate_limiter.acquire()
        
        # ë‚ ì§œ ê¸°ë³¸ê°’ ì„¤ì •
        if not from_date:
            from_date = datetime.now().strftime("%Y%m%d")
        if not to_date:
            to_date = (datetime.now() + timedelta(days=30)).strftime("%Y%m%d")
        
        # ë‚ ì§œ ìœ íš¨ì„± ê²€ì¦
        if not self._validate_date_format(from_date) or not self._validate_date_format(to_date):
            raise ValueError("ë‚ ì§œ í˜•ì‹ì€ YYYYMMDD ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        if not self._validate_date_range(from_date, to_date):
            raise ValueError("ì‹œì‘ì¼ì€ ì¢…ë£Œì¼ë³´ë‹¤ ì´ì „ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        path = "uapi/domestic-stock/v1/ksdinfo/pub-offer"
        url = f"{self.base_url}/{path}"
        headers = {
            "content-type": "application/json",
            "authorization": self.access_token,
            "appKey": self.api_key,
            "appSecret": self.api_secret,
            "tr_id": "HHKDB669108C0",
            "custtype": "P"  # ê°œì¸
        }
        
        params = {
            "SHT_CD": symbol,
            "CTS": "",
            "F_DT": from_date,
            "T_DT": to_date
        }
        
        resp = requests.get(url, headers=headers, params=params)
        resp_json = resp.json()
        
        # ì—ëŸ¬ ì²˜ë¦¬
        if resp_json.get('rt_cd') != '0':
            logger.error(f"ê³µëª¨ì£¼ ì¡°íšŒ ì‹¤íŒ¨: {resp_json.get('msg1', 'Unknown error')}")
            return resp_json
        
        return resp_json


# RateLimiter í´ë˜ìŠ¤ëŠ” enhanced_rate_limiter.pyë¡œ ì´ë™ë¨


if __name__ == "__main__":
    with open("../koreainvestment.key", encoding='utf-8') as key_file:
        lines = key_file.readlines()

    key = lines[0].strip()
    secret = lines[1].strip()
    acc_no = lines[2].strip()

    broker = KoreaInvestment(
        api_key=key,
        api_secret=secret,
        acc_no=acc_no,
        # exchange="ë‚˜ìŠ¤ë‹¥" # todo: exchangeëŠ” ì œê±° ì˜ˆì •
    )

    balance = broker.fetch_present_balance()
    print(balance)

    # result = broker.fetch_oversea_day_night()
    # pprint.pprint(result)

    # minute1_ohlcv = broker.fetch_today_1m_ohlcv("005930")
    # pprint.pprint(minute1_ohlcv)

    # broker = KoreaInvestment(key, secret, exchange="ë‚˜ìŠ¤ë‹¥")
    # import pprint
    # resp = broker.fetch_price("005930")
    # pprint.pprint(resp)
    #
    # b = broker.fetch_balance("63398082")
    # pprint.pprint(b)
    #
    # resp = broker.create_market_buy_order("63398082", "005930", 10)
    # pprint.pprint(resp)
    #
    # resp = broker.cancel_order("63398082", "91252", "0000117057", "00", 60000, 5, "Y")
    # print(resp)
    #
    # resp = broker.create_limit_buy_order("63398082", "TQQQ", 35, 1)
    # print(resp)



    # import pprint
    # broker = KoreaInvestment(key, secret, exchange="ë‚˜ìŠ¤ë‹¥")
    # resp_ohlcv = broker.fetch_ohlcv("TSLA", '1d', to="")
    # print(len(resp_ohlcv['output2']))
    # pprint.pprint(resp_ohlcv['output2'][0])
    # pprint.pprint(resp_ohlcv['output2'][-1])
