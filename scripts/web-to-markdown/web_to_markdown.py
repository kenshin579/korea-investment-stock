#!/usr/bin/env python3
"""
ì›¹ì‚¬ì´íŠ¸ë¥¼ AI ì¹œí™”ì ì¸ Markdownìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
Crawl4AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹í˜ì´ì§€ì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ ë³´ì¡´í•˜ë©´ì„œ Markdownìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

import asyncio
import argparse
import os
import sys
from datetime import datetime
from urllib.parse import urlparse, urljoin
import json
from typing import Optional, Dict, Any

from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import LLMExtractionStrategy, JsonCssExtractionStrategy
import markdownify


class WebToMarkdownConverter:
    """ì›¹ì‚¬ì´íŠ¸ë¥¼ AI ì¹œí™”ì ì¸ Markdownìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, output_dir: str = "markdown_output"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
    async def crawl_and_convert(self, url: str, max_depth: int = 1, 
                               include_metadata: bool = True,
                               include_links: bool = True,
                               include_images: bool = True) -> Dict[str, Any]:
        """
        ì›¹ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ê³  Markdownìœ¼ë¡œ ë³€í™˜
        
        Args:
            url: í¬ë¡¤ë§í•  URL
            max_depth: í¬ë¡¤ë§ ê¹Šì´ (ê¸°ë³¸ê°’: 1)
            include_metadata: ë©”íƒ€ë°ì´í„° í¬í•¨ ì—¬ë¶€
            include_links: ë§í¬ í¬í•¨ ì—¬ë¶€
            include_images: ì´ë¯¸ì§€ í¬í•¨ ì—¬ë¶€
            
        Returns:
            í¬ë¡¤ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        results = {}
        
        async with AsyncWebCrawler(verbose=True) as crawler:
            # ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§
            result = await crawler.arun(
                url=url,
                word_count_threshold=10,
                exclude_external_links=True,
                remove_overlay=True,
                process_iframes=True
            )
            
            if result.success:
                # Markdown ë³€í™˜
                markdown_content = self._convert_to_markdown(
                    result,
                    include_metadata=include_metadata,
                    include_links=include_links,
                    include_images=include_images
                )
                
                # íŒŒì¼ ì €ì¥
                filename = self._save_markdown(url, markdown_content)
                
                results[url] = {
                    'success': True,
                    'filename': filename,
                    'content_length': len(markdown_content),
                    'title': result.metadata.get('title', 'No title'),
                    'description': result.metadata.get('description', 'No description')
                }
                
                # ë§í¬ëœ í˜ì´ì§€ë“¤ë„ í¬ë¡¤ë§ (max_depthì— ë”°ë¼)
                if max_depth > 1 and result.links:
                    for link in result.links[:10]:  # ìµœëŒ€ 10ê°œ ë§í¬ë§Œ
                        if link.startswith('http'):
                            sub_result = await self._crawl_subpage(crawler, link, url)
                            if sub_result:
                                results[link] = sub_result
            else:
                results[url] = {
                    'success': False,
                    'error': result.error_message
                }
                
        return results
    
    async def _crawl_subpage(self, crawler, url: str, base_url: str) -> Optional[Dict[str, Any]]:
        """ì„œë¸Œí˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            result = await crawler.arun(
                url=url,
                word_count_threshold=10,
                exclude_external_links=True,
                remove_overlay=True
            )
            
            if result.success:
                markdown_content = self._convert_to_markdown(result)
                filename = self._save_markdown(url, markdown_content)
                
                return {
                    'success': True,
                    'filename': filename,
                    'content_length': len(markdown_content),
                    'title': result.metadata.get('title', 'No title')
                }
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
        
        return None
    
    def _convert_to_markdown(self, crawl_result, 
                           include_metadata: bool = True,
                           include_links: bool = True,
                           include_images: bool = True) -> str:
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ AI ì¹œí™”ì ì¸ Markdownìœ¼ë¡œ ë³€í™˜"""
        markdown_parts = []
        
        # ë©”íƒ€ë°ì´í„° í—¤ë”
        if include_metadata:
            markdown_parts.append("---")
            markdown_parts.append(f"title: {crawl_result.metadata.get('title', 'Untitled')}")
            markdown_parts.append(f"url: {crawl_result.url}")
            markdown_parts.append(f"crawled_at: {datetime.now().isoformat()}")
            if crawl_result.metadata.get('description'):
                markdown_parts.append(f"description: {crawl_result.metadata.get('description')}")
            if crawl_result.metadata.get('keywords'):
                markdown_parts.append(f"keywords: {crawl_result.metadata.get('keywords')}")
            markdown_parts.append("---")
            markdown_parts.append("")
        
        # ì œëª©
        title = crawl_result.metadata.get('title', 'Untitled')
        markdown_parts.append(f"# {title}")
        markdown_parts.append("")
        
        # ë©”ì¸ ì½˜í…ì¸ 
        if crawl_result.markdown:
            # Crawl4AIê°€ ì´ë¯¸ markdownìœ¼ë¡œ ë³€í™˜í•œ ê²½ìš°
            content = crawl_result.markdown
        else:
            # HTMLì„ markdownìœ¼ë¡œ ë³€í™˜
            content = markdownify.markdownify(
                crawl_result.html,
                heading_style="ATX",
                bullets="-",
                code_language="python"
            )
        
        # ì½˜í…ì¸  ì •ë¦¬
        content = self._clean_markdown(content)
        markdown_parts.append(content)
        
        # ì´ë¯¸ì§€ ëª©ë¡
        if include_images and crawl_result.media.get('images'):
            markdown_parts.append("\n## ì´ë¯¸ì§€ ëª©ë¡")
            markdown_parts.append("")
            for img in crawl_result.media['images']:
                img_url = img.get('src', '')
                alt_text = img.get('alt', 'Image')
                if img_url:
                    markdown_parts.append(f"- ![{alt_text}]({img_url})")
        
        # ë§í¬ ëª©ë¡
        if include_links and crawl_result.links:
            markdown_parts.append("\n## ì°¸ì¡° ë§í¬")
            markdown_parts.append("")
            unique_links = list(set(crawl_result.links))[:20]  # ìµœëŒ€ 20ê°œ
            for link in unique_links:
                if link.startswith('http'):
                    markdown_parts.append(f"- {link}")
        
        return "\n".join(markdown_parts)
    
    def _clean_markdown(self, content: str) -> str:
        """Markdown ì½˜í…ì¸  ì •ë¦¬"""
        # ì¤‘ë³µëœ ì¤„ë°”ê¿ˆ ì œê±°
        lines = content.split('\n')
        cleaned_lines = []
        empty_count = 0
        
        for line in lines:
            if line.strip() == '':
                empty_count += 1
                if empty_count <= 2:  # ìµœëŒ€ 2ê°œì˜ ë¹ˆ ì¤„ë§Œ í—ˆìš©
                    cleaned_lines.append(line)
            else:
                empty_count = 0
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def _save_markdown(self, url: str, content: str) -> str:
        """Markdown íŒŒì¼ë¡œ ì €ì¥"""
        # URLì—ì„œ íŒŒì¼ëª… ìƒì„±
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.replace('.', '_')
        path = parsed_url.path.strip('/').replace('/', '_')
        
        if not path:
            path = 'index'
        
        filename = f"{domain}_{path}.md"
        filepath = os.path.join(self.output_dir, filename)
        
        # íŒŒì¼ëª… ì¤‘ë³µ ì²˜ë¦¬
        counter = 1
        while os.path.exists(filepath):
            filename = f"{domain}_{path}_{counter}.md"
            filepath = os.path.join(self.output_dir, filename)
            counter += 1
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"âœ… Saved: {filepath}")
        return filepath


async def main():
    parser = argparse.ArgumentParser(
        description="ì›¹ì‚¬ì´íŠ¸ë¥¼ AI ì¹œí™”ì ì¸ Markdownìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
    )
    parser.add_argument("url", help="ë³€í™˜í•  ì›¹ì‚¬ì´íŠ¸ URL")
    parser.add_argument("-o", "--output", default="markdown_output", 
                       help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: markdown_output)")
    parser.add_argument("-d", "--depth", type=int, default=1,
                       help="í¬ë¡¤ë§ ê¹Šì´ (ê¸°ë³¸ê°’: 1)")
    parser.add_argument("--no-metadata", action="store_true",
                       help="ë©”íƒ€ë°ì´í„° ì œì™¸")
    parser.add_argument("--no-links", action="store_true",
                       help="ë§í¬ ëª©ë¡ ì œì™¸")
    parser.add_argument("--no-images", action="store_true",
                       help="ì´ë¯¸ì§€ ëª©ë¡ ì œì™¸")
    
    args = parser.parse_args()
    
    # URL ê²€ì¦
    if not args.url.startswith(('http://', 'https://')):
        args.url = 'https://' + args.url
    
    print(f"ğŸŒ í¬ë¡¤ë§ ì‹œì‘: {args.url}")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output}")
    print(f"ğŸ” í¬ë¡¤ë§ ê¹Šì´: {args.depth}")
    
    converter = WebToMarkdownConverter(output_dir=args.output)
    
    try:
        results = await converter.crawl_and_convert(
            url=args.url,
            max_depth=args.depth,
            include_metadata=not args.no_metadata,
            include_links=not args.no_links,
            include_images=not args.no_images
        )
        
        # ê²°ê³¼ ìš”ì•½
        print("\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼:")
        success_count = sum(1 for r in results.values() if r['success'])
        print(f"âœ… ì„±ê³µ: {success_count}ê°œ í˜ì´ì§€")
        
        if len(results) > success_count:
            print(f"âŒ ì‹¤íŒ¨: {len(results) - success_count}ê°œ í˜ì´ì§€")
        
        # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œë„ ì €ì¥
        summary_file = os.path.join(args.output, "crawl_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“„ ìš”ì•½ íŒŒì¼: {summary_file}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


def run():
    """ëª…ë ¹ì¤„ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ë¥¼ ìœ„í•œ ë™ê¸° ë˜í¼"""
    asyncio.run(main())

if __name__ == "__main__":
    run() 