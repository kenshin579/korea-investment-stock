#!/usr/bin/env python3
"""
AI ê¸°ë°˜ ì›¹ to Markdown ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸
LLMì„ í™œìš©í•˜ì—¬ ë” ìŠ¤ë§ˆíŠ¸í•œ ì½˜í…ì¸  ì¶”ì¶œ ë° êµ¬ì¡°í™”
"""

import asyncio
import argparse
import os
import json
from datetime import datetime
from typing import Dict, Any, List
from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import LLMExtractionStrategy


class AIWebToMarkdown:
    """AIë¥¼ í™œìš©í•œ ì›¹ to Markdown ë³€í™˜ê¸°"""
    
    def __init__(self, output_dir: str = "ai_markdown_output"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
    async def extract_with_ai(self, url: str, extraction_type: str = "article") -> Dict[str, Any]:
        """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹í˜ì´ì§€ì—ì„œ êµ¬ì¡°í™”ëœ ì½˜í…ì¸  ì¶”ì¶œ"""
        
        # ì¶”ì¶œ íƒ€ì…ë³„ ìŠ¤í‚¤ë§ˆ ì •ì˜
        schemas = {
            "article": {
                "title": "string",
                "author": "string",
                "date": "string",
                "summary": "string",
                "main_content": "string",
                "key_points": ["string"],
                "tags": ["string"]
            },
            "product": {
                "name": "string",
                "price": "string",
                "description": "string",
                "features": ["string"],
                "specifications": {"key": "value"},
                "reviews_summary": "string"
            },
            "documentation": {
                "title": "string",
                "overview": "string",
                "sections": [
                    {
                        "heading": "string",
                        "content": "string",
                        "code_examples": ["string"]
                    }
                ],
                "prerequisites": ["string"],
                "related_topics": ["string"]
            },
            "general": {
                "title": "string",
                "main_topic": "string",
                "summary": "string",
                "key_sections": [
                    {
                        "heading": "string",
                        "content": "string"
                    }
                ],
                "important_links": ["string"],
                "metadata": {"key": "value"}
            }
        }
        
        # LLM ì¶”ì¶œ ì „ëµ ì„¤ì •
        extraction_strategy = LLMExtractionStrategy(
            provider="openai/gpt-4",  # ì‹¤ì œ ì‚¬ìš©ì‹œ API í‚¤ í•„ìš”
            api_token=os.getenv("OPENAI_API_KEY"),
            schema=schemas.get(extraction_type, schemas["general"]),
            extraction_type="schema",
            instruction=f"""
            ì›¹í˜ì´ì§€ì—ì„œ {extraction_type} í˜•ì‹ì˜ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
            ê°€ëŠ¥í•œ í•œ ìì„¸í•˜ê³  êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì œê³µí•˜ë˜,
            ì›ë³¸ ì½˜í…ì¸ ì˜ ì˜ë¯¸ë¥¼ ì •í™•íˆ ë³´ì¡´í•˜ì„¸ìš”.
            ì½”ë“œ ì˜ˆì œê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ í¬í•¨ì‹œí‚¤ì„¸ìš”.
            """
        )
        
        async with AsyncWebCrawler(verbose=True) as crawler:
            result = await crawler.arun(
                url=url,
                extraction_strategy=extraction_strategy,
                bypass_cache=True
            )
            
            return result
    
    async def smart_crawl_and_convert(self, url: str, 
                                    extraction_type: str = "general",
                                    include_raw: bool = False) -> str:
        """AIë¥¼ í™œìš©í•œ ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ ë° ë³€í™˜"""
        
        print(f"ğŸ¤– AI ê¸°ë°˜ ì½˜í…ì¸  ì¶”ì¶œ ì¤‘... (íƒ€ì…: {extraction_type})")
        
        # ê¸°ë³¸ í¬ë¡¤ë§ (ë°±ì—…ìš©)
        async with AsyncWebCrawler(verbose=True) as crawler:
            basic_result = await crawler.arun(
                url=url,
                word_count_threshold=10,
                exclude_external_links=True,
                remove_overlay=True
            )
        
        # AI ì¶”ì¶œ ì‹œë„
        ai_extracted = None
        try:
            if os.getenv("OPENAI_API_KEY"):
                ai_result = await self.extract_with_ai(url, extraction_type)
                if ai_result.success and ai_result.extracted_content:
                    ai_extracted = json.loads(ai_result.extracted_content)
        except Exception as e:
            print(f"âš ï¸ AI ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ ì¶”ì¶œ ì‚¬ìš©: {e}")
        
        # Markdown ìƒì„±
        markdown_content = self._create_ai_enhanced_markdown(
            basic_result,
            ai_extracted,
            extraction_type,
            include_raw
        )
        
        # íŒŒì¼ ì €ì¥
        filename = self._save_markdown(url, markdown_content, extraction_type)
        
        return filename
    
    def _create_ai_enhanced_markdown(self, basic_result, ai_data, 
                                   extraction_type: str, include_raw: bool) -> str:
        """AI ì¶”ì¶œ ë°ì´í„°ë¥¼ í™œìš©í•œ í–¥ìƒëœ Markdown ìƒì„±"""
        
        parts = []
        
        # í—¤ë”
        parts.append("---")
        parts.append(f"title: {basic_result.metadata.get('title', 'Untitled')}")
        parts.append(f"url: {basic_result.url}")
        parts.append(f"extracted_at: {datetime.now().isoformat()}")
        parts.append(f"extraction_type: {extraction_type}")
        parts.append(f"ai_enhanced: {'yes' if ai_data else 'no'}")
        parts.append("---")
        parts.append("")
        
        # AI ì¶”ì¶œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
        if ai_data:
            parts.append("# " + ai_data.get('title', basic_result.metadata.get('title', 'Untitled')))
            parts.append("")
            
            # íƒ€ì…ë³„ íŠ¹ìˆ˜ ì²˜ë¦¬
            if extraction_type == "article":
                self._format_article(parts, ai_data)
            elif extraction_type == "product":
                self._format_product(parts, ai_data)
            elif extraction_type == "documentation":
                self._format_documentation(parts, ai_data)
            else:
                self._format_general(parts, ai_data)
        else:
            # AI ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ í¬ë§·
            parts.append("# " + basic_result.metadata.get('title', 'Untitled'))
            parts.append("")
            if basic_result.markdown:
                parts.append(basic_result.markdown)
            else:
                parts.append(basic_result.cleaned_text)
        
        # ì›ë³¸ ì½˜í…ì¸  í¬í•¨ ì˜µì…˜
        if include_raw and not ai_data:
            parts.append("\n---\n")
            parts.append("## ì›ë³¸ ì½˜í…ì¸ ")
            parts.append("")
            parts.append("```")
            parts.append(basic_result.cleaned_text[:5000])  # ì²˜ìŒ 5000ìë§Œ
            parts.append("```")
        
        # ë©”íƒ€ë°ì´í„°
        parts.append("\n---\n")
        parts.append("## í˜ì´ì§€ ì •ë³´")
        parts.append("")
        parts.append(f"- **URL**: {basic_result.url}")
        parts.append(f"- **ì œëª©**: {basic_result.metadata.get('title', 'N/A')}")
        parts.append(f"- **ì„¤ëª…**: {basic_result.metadata.get('description', 'N/A')}")
        
        return "\n".join(parts)
    
    def _format_article(self, parts: List[str], data: Dict[str, Any]):
        """ê¸°ì‚¬ í˜•ì‹ í¬ë§·íŒ…"""
        if data.get('author'):
            parts.append(f"**ì‘ì„±ì**: {data['author']}")
        if data.get('date'):
            parts.append(f"**ë‚ ì§œ**: {data['date']}")
        parts.append("")
        
        if data.get('summary'):
            parts.append("## ìš”ì•½")
            parts.append("")
            parts.append(data['summary'])
            parts.append("")
        
        if data.get('main_content'):
            parts.append("## ë³¸ë¬¸")
            parts.append("")
            parts.append(data['main_content'])
            parts.append("")
        
        if data.get('key_points'):
            parts.append("## í•µì‹¬ í¬ì¸íŠ¸")
            parts.append("")
            for point in data['key_points']:
                parts.append(f"- {point}")
            parts.append("")
        
        if data.get('tags'):
            parts.append("**íƒœê·¸**: " + ", ".join(f"`{tag}`" for tag in data['tags']))
    
    def _format_product(self, parts: List[str], data: Dict[str, Any]):
        """ì œí’ˆ ì •ë³´ í¬ë§·íŒ…"""
        if data.get('price'):
            parts.append(f"**ê°€ê²©**: {data['price']}")
        parts.append("")
        
        if data.get('description'):
            parts.append("## ì œí’ˆ ì„¤ëª…")
            parts.append("")
            parts.append(data['description'])
            parts.append("")
        
        if data.get('features'):
            parts.append("## ì£¼ìš” íŠ¹ì§•")
            parts.append("")
            for feature in data['features']:
                parts.append(f"- {feature}")
            parts.append("")
        
        if data.get('specifications'):
            parts.append("## ì‚¬ì–‘")
            parts.append("")
            parts.append("| í•­ëª© | ë‚´ìš© |")
            parts.append("|------|------|")
            for key, value in data['specifications'].items():
                parts.append(f"| {key} | {value} |")
            parts.append("")
    
    def _format_documentation(self, parts: List[str], data: Dict[str, Any]):
        """ë¬¸ì„œ í˜•ì‹ í¬ë§·íŒ…"""
        if data.get('overview'):
            parts.append("## ê°œìš”")
            parts.append("")
            parts.append(data['overview'])
            parts.append("")
        
        if data.get('prerequisites'):
            parts.append("## ì‚¬ì „ ìš”êµ¬ì‚¬í•­")
            parts.append("")
            for prereq in data['prerequisites']:
                parts.append(f"- {prereq}")
            parts.append("")
        
        if data.get('sections'):
            for section in data['sections']:
                parts.append(f"## {section['heading']}")
                parts.append("")
                parts.append(section['content'])
                
                if section.get('code_examples'):
                    parts.append("")
                    for code in section['code_examples']:
                        parts.append("```")
                        parts.append(code)
                        parts.append("```")
                        parts.append("")
    
    def _format_general(self, parts: List[str], data: Dict[str, Any]):
        """ì¼ë°˜ í˜•ì‹ í¬ë§·íŒ…"""
        if data.get('main_topic'):
            parts.append(f"**ì£¼ì œ**: {data['main_topic']}")
            parts.append("")
        
        if data.get('summary'):
            parts.append("## ìš”ì•½")
            parts.append("")
            parts.append(data['summary'])
            parts.append("")
        
        if data.get('key_sections'):
            for section in data['key_sections']:
                parts.append(f"## {section['heading']}")
                parts.append("")
                parts.append(section['content'])
                parts.append("")
    
    def _save_markdown(self, url: str, content: str, extraction_type: str) -> str:
        """Markdown íŒŒì¼ ì €ì¥"""
        from urllib.parse import urlparse
        
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.replace('.', '_')
        path = parsed_url.path.strip('/').replace('/', '_') or 'index'
        
        filename = f"{domain}_{path}_{extraction_type}.md"
        filepath = os.path.join(self.output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {filepath}")
        return filepath


async def main():
    parser = argparse.ArgumentParser(
        description="AIë¥¼ í™œìš©í•œ ì›¹ì‚¬ì´íŠ¸ to Markdown ë³€í™˜ê¸°"
    )
    parser.add_argument("url", help="ë³€í™˜í•  ì›¹ì‚¬ì´íŠ¸ URL")
    parser.add_argument("-t", "--type", 
                       choices=["article", "product", "documentation", "general"],
                       default="general",
                       help="ì¶”ì¶œ íƒ€ì… (ê¸°ë³¸ê°’: general)")
    parser.add_argument("-o", "--output", default="ai_markdown_output",
                       help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--include-raw", action="store_true",
                       help="ì›ë³¸ ì½˜í…ì¸  í¬í•¨")
    
    args = parser.parse_args()
    
    # URL ê²€ì¦
    if not args.url.startswith(('http://', 'https://')):
        args.url = 'https://' + args.url
    
    print(f"ğŸŒ URL: {args.url}")
    print(f"ğŸ¤– ì¶”ì¶œ íƒ€ì…: {args.type}")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output}")
    
    # API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸ ì£¼ì˜: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ì¶”ì¶œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        print("AI ê¸°ë°˜ ì¶”ì¶œì„ ì‚¬ìš©í•˜ë ¤ë©´ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”:")
        print("export OPENAI_API_KEY='your-api-key'")
    
    converter = AIWebToMarkdown(output_dir=args.output)
    
    try:
        filename = await converter.smart_crawl_and_convert(
            url=args.url,
            extraction_type=args.type,
            include_raw=args.include_raw
        )
        print(f"\nâœ¨ ë³€í™˜ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


def run():
    """ëª…ë ¹ì¤„ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ë¥¼ ìœ„í•œ ë™ê¸° ë˜í¼"""
    asyncio.run(main())

if __name__ == "__main__":
    run() 